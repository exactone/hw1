{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1450 training set, 100 testing set\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "data_dir = './MLDS_hw2_data/'\n",
    "test_out_dir = './test_out_dir/'\n",
    "peer_out_dir = './peer_out_dir/'\n",
    "\n",
    "train_feat_list = glob.glob(data_dir+'training_data/feat/*')\n",
    "test_feat_list = glob.glob(data_dir+'testing_data/feat/*')\n",
    "print(len(train_feat_list), 'training set,', len(test_feat_list), 'testing set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training features ...\n",
      "train_data.shape: (1450, 80, 4096) train_data.dtype: float64\n",
      "pickling train_data\n"
     ]
    }
   ],
   "source": [
    "print('loading training features ...')\n",
    "train_data = list()\n",
    "for train_feat in train_feat_list:\n",
    "    train_data.append(np.load(train_feat))\n",
    "else:\n",
    "    train_data = np.array(train_data)\n",
    "    print('train_data.shape:', train_data.shape, 'train_data.dtype:', train_data.dtype)\n",
    "    print('pickling train_data')\n",
    "    pickle.dump(train_data, open('train_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading testing features ...\n",
      "test_data.shape: (100, 80, 4096) test_data.dtype: float64\n",
      "pickling test_data\n"
     ]
    }
   ],
   "source": [
    "print('loading testing features ...')\n",
    "test_data = list()\n",
    "for test_feat in test_feat_list:\n",
    "    test_data.append(np.load(test_feat))\n",
    "else:\n",
    "    test_data = np.array(test_data)\n",
    "    print('test_data.shape:', test_data.shape, 'test_data.dtype:', test_data.dtype)\n",
    "    print('pickling test_data')\n",
    "    pickle.dump(test_data, open('test_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train_data, test_data from .pkl ...\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "print('loading train_data, test_data from .pkl ...')\n",
    "train_data = pickle.load(open('train_data.pkl', 'rb'))\n",
    "test_data = pickle.load(open('test_data.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training label ...\n",
      "loading training id ...\n",
      "loading testing label ...\n",
      "loading testing id ...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print('loading training label ...')\n",
    "train_label_json = json.load(open(data_dir+'training_label.json'))\n",
    "print('loading training id ...')\n",
    "training_id = [x.split('/')[-1].replace('.npy','') for x in train_feat_list]\n",
    "\n",
    "print('loading testing label ...')\n",
    "test_label_json = json.load(open(data_dir+'testing_label.json'))\n",
    "print('loading testing id ...')\n",
    "testing_id = [x.split('/')[-1].replace('.npy','') for x in test_feat_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 6346 words in training/testing label\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "caption_words = set()\n",
    "caption_dict = {'2':'two', '3':'three', '4':'four', '5':'five', '6':'six'}\n",
    "for tj in train_label_json + test_label_json:\n",
    "    for c in tj['caption']:\n",
    "        c = c.replace('one hundred', '100')\n",
    "        c = c.replace('two hundred', '200')\n",
    "        s = [re.sub('[\\\",.;?!%”“]', '', s.lower()) for s in c.split(' ')]\n",
    "        \n",
    "        s[-1] = s[-1].replace('.', '')\n",
    "        caption_words |= set(s)\n",
    "else:\n",
    "    caption_words.add('<BOS>')\n",
    "    caption_words.add('<EOS>')\n",
    "    caption_words.add('<NULL>')\n",
    "    \n",
    "    caption_words.remove('2')\n",
    "    caption_words.remove('3')\n",
    "    caption_words.remove('4')\n",
    "    caption_words.remove('5')\n",
    "    caption_words.remove('6')\n",
    "            \n",
    "    print('there are', len(caption_words), 'words in training/testing label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caption_words label encoding...\n",
      "encoding 6346 classes\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "print('caption_words label encoding...')\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(list(caption_words))\n",
    "print('encoding', len(le.classes_), 'classes')\n",
    "#le.transform([1, 1, 2, 6]) \n",
    "#le.inverse_transform([0, 0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 hit at i: 1069 replaced by two\n",
      "2 hit at i: 1449 replaced by two\n",
      "4 hit at i: 659 replaced by four\n",
      "4 hit at i: 659 replaced by four\n",
      "3 hit at i: 25 replaced by three\n",
      "6 hit at i: 931 replaced by six\n",
      "5 hit at i: 1429 replaced by five\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "train_label_json_split = copy.deepcopy(train_label_json)\n",
    "for tid in training_id:\n",
    "    for i, tj in enumerate(train_label_json_split):\n",
    "        if tid == tj['id']:\n",
    "            caption = list()\n",
    "            caption_one_hot = list()\n",
    "            caption_mask = list()\n",
    "            for c in tj['caption']:\n",
    "                c = c.replace('one hundred', '100')\n",
    "                c = c.replace('two hundred', '200')\n",
    "                s = [re.sub('[\\\",.;?!%”“]', '', s.lower()) for s in c.split(' ')]\n",
    "                \n",
    "                # 將2, 3, 4, 5, 6用two, three, four, five, six取代\n",
    "                for si, ss in enumerate(s):\n",
    "                    if ss in caption_dict:\n",
    "                        print(ss, 'hit at i:', i, 'replaced by', caption_dict[ss])\n",
    "                        s[si] = caption_dict[ss]\n",
    "                        \n",
    "                s.insert(0, '<BOS>')\n",
    "                s.append('<EOS>')\n",
    "                for si in range(len(s), 37):\n",
    "                    s.append('<NULL>')\n",
    "                caption.append(s)\n",
    "                s_one_hot = le.transform(s)\n",
    "                caption_one_hot.append(s_one_hot)\n",
    "                caption_mask.append(s_one_hot == le.transform(['<NULL>']))\n",
    "            else:\n",
    "                train_label_json_split[i]['caption'] = caption\n",
    "                train_label_json_split[i]['one_hot'] = caption_one_hot\n",
    "                train_label_json_split[i]['mask'] = caption_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "test_label_json_split = copy.deepcopy(test_label_json)\n",
    "for tid in testing_id:\n",
    "    for i, tj in enumerate(test_label_json_split):\n",
    "        if tid == tj['id']:\n",
    "            caption = list()\n",
    "            caption_one_hot = list()\n",
    "            caption_mask = list()\n",
    "            for c in tj['caption']:\n",
    "                c = c.replace('one hundred', '100')\n",
    "                c = c.replace('two hundred', '200')\n",
    "                s = [re.sub('[\\\",.;?!%”“]', '', s.lower()) for s in c.split(' ')]\n",
    "                \n",
    "                # 將2, 3, 4, 5, 6用two, three, four, five, six取代\n",
    "                for si, ss in enumerate(s):\n",
    "                    if ss in caption_dict:\n",
    "                        print(ss, 'hit at i:', i, 'replaced by', caption_dict[ss])\n",
    "                        s[si] = caption_dict[ss]\n",
    "                        \n",
    "                s.insert(0, '<BOS>')\n",
    "                s.append('<EOS>')\n",
    "                for si in range(len(s), 37):\n",
    "                    s.append('<NULL>')\n",
    "                caption.append(s)\n",
    "                s_one_hot = le.transform(s)\n",
    "                caption_one_hot.append(s_one_hot)\n",
    "                caption_mask.append(s_one_hot == le.transform(['<NULL>']))\n",
    "            else:\n",
    "                test_label_json_split[i]['caption'] = caption\n",
    "                test_label_json_split[i]['one_hot'] = caption_one_hot\n",
    "                test_label_json_split[i]['mask'] = caption_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of caption is 37\n"
     ]
    }
   ],
   "source": [
    "print('max length of caption is', 37)\n",
    "#max_ = 0\n",
    "#for t in train_label_json_split+test_label_json_split:\n",
    "#    if len(t['caption']) > max_:\n",
    "#        max_ = len(t['caption'])\n",
    "#else:\n",
    "#    print('max length of caption is', max_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding phase length: 1450  decoding phase length: 37\n",
      "pad train_data from (80, 4096) to ( 117 , 4096 )\n",
      "shape after padding: (1450, 117, 4096)\n"
     ]
    }
   ],
   "source": [
    "print('encoding phase length:', train_data.shape[0], ' decoding phase length:', 37)\n",
    "print('pad train_data from', train_data.shape[1:], 'to (', train_data.shape[1]+37, ',', train_data.shape[2],')') \n",
    "\n",
    "train_data_padding = list()\n",
    "for t in train_data:\n",
    "    train_data_padding.append(np.concatenate([t, np.zeros((37, 4096))]))\n",
    "else:\n",
    "    train_data_padding = np.array(train_data_padding)\n",
    "    print('shape after padding:', train_data_padding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding phase length: 100  decoding phase length: 37\n",
      "pad test_data from (80, 4096) to ( 117 , 4096 )\n",
      "shape after padding: (100, 117, 4096)\n"
     ]
    }
   ],
   "source": [
    "print('encoding phase length:', test_data.shape[0], ' decoding phase length:', 37)\n",
    "print('pad test_data from', test_data.shape[1:], 'to (', test_data.shape[1]+37, ',', test_data.shape[2],')') \n",
    "\n",
    "test_data_padding = list()\n",
    "for t in test_data:\n",
    "    test_data_padding.append(np.concatenate([t, np.zeros((37, 4096))]))\n",
    "else:\n",
    "    test_data_padding = np.array(test_data_padding)\n",
    "    print('shape after padding:', test_data_padding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_label_padding.shape: (1450, 117, 1) train_label_mask.shape: (1450, 117, 1)\n",
      "test_label_padding.shape: (100, 117, 1) test_label_mask.shape: (100, 117, 1)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "train_label_padding = list()\n",
    "train_label_mask = list()\n",
    "\n",
    "test_label_padding = list()\n",
    "test_label_mask = list()\n",
    "def random_pick_gen_label():\n",
    "    global train_label_padding\n",
    "    global train_label_mask\n",
    "    global test_label_padding\n",
    "    global test_label_mask\n",
    "#if True:    \n",
    "    for i, tid in enumerate(training_id):\n",
    "        for tj in train_label_json_split:\n",
    "            if tid == tj['id']:\n",
    "                k = random.randint(0, len(tj['one_hot'])-1)\n",
    "                \n",
    "                pad80_label37 = np.concatenate((np.full((80, ), le.transform(['<NULL>'])), tj['one_hot'][k]))\n",
    "                pad80_label37_bool = np.concatenate((np.full((80, ), False), tj['mask'][k]))\n",
    "                \n",
    "                train_label_padding.append(pad80_label37.reshape((117,1)))\n",
    "                train_label_mask.append(pad80_label37_bool.reshape((117,1)))\n",
    "    else:\n",
    "        train_label_padding = np.array(train_label_padding)\n",
    "        train_label_padding = train_label_padding.reshape((1450, 117, 1))\n",
    "        train_label_mask = np.array(train_label_mask)\n",
    "        train_label_mask = train_label_mask.reshape((1450, 117, 1))\n",
    "        \n",
    "                \n",
    "\n",
    "    for i, tid in enumerate(testing_id):\n",
    "        for tj in test_label_json_split:\n",
    "            if tid == tj['id']:\n",
    "                k = random.randint(0, len(tj['one_hot'])-1)\n",
    "                \n",
    "                pad80_label37 = np.concatenate((np.full((80, ), le.transform(['<NULL>'])), tj['one_hot'][k]))\n",
    "                pad80_label37_bool = np.concatenate((np.full((80, ), False), tj['mask'][k]))\n",
    "                \n",
    "                test_label_padding.append(pad80_label37.reshape((117,1)))\n",
    "                test_label_mask.append(pad80_label37_bool.reshape((117,1)))\n",
    "    else:\n",
    "        test_label_padding = np.array(test_label_padding)\n",
    "        test_label_padding = test_label_padding.reshape((100, 117, 1))\n",
    "        test_label_mask = np.array(test_label_mask)\n",
    "        test_label_mask = test_label_mask.reshape((100, 117, 1))\n",
    "        \n",
    "\n",
    "random_pick_gen_label()\n",
    "print('train_label_padding.shape:', train_label_padding.shape, 'train_label_mask.shape:', train_label_mask.shape)\n",
    "print('test_label_padding.shape:', test_label_padding.shape, 'test_label_mask.shape:', test_label_mask.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import keras.backend as K \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.layers import GRU\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, return_sequences=True, activation='sigmoid', dropout=0.25), input_shape=(sample_length, dim))\n",
    "model.add(LSTM(256, return_sequences=True, activation='sigmoid', dropout=0.25), input_shape=(sample_length, dim))\n",
    "                     \n",
    "            model.add(Dense(256,activation='sigmoid'))\n",
    "            model.add(Dropout(0.25))\n",
    "            model.add(Dense(64,activation='sigmoid'))\n",
    "            model.add(Dropout(0.25))\n",
    "            model.add(Dense(48,activation='softmax'))\n",
    "            model.summary()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
