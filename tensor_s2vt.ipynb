{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/matplotlib/font_manager.py:280: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import re\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "video_train_feat_path = './rgb_train_features'\n",
    "video_train_data_path = './data/video_corpus.csv'\n",
    "video_test_feat_path = './rgb_test_features'\n",
    "video_test_data_path = './data/video_corpus.csv'\n",
    "\n",
    "dim_image = 4096\n",
    "dim_hidden= 256\n",
    "\n",
    "n_video_lstm_step = 80\n",
    "n_caption_lstm_step = 20\n",
    "n_frame_step = 80\n",
    "\n",
    "n_epochs = 200\n",
    "batch_size = 50\n",
    "learning_rate = 0.0001\n",
    "\n",
    "def get_video_train_data(video_data_path, video_feat_path):\n",
    "    video_data = pd.read_csv(video_data_path, sep=',')\n",
    "    video_data = video_data[video_data['Language'] == 'English']\n",
    "    video_data['video_path'] = video_data.apply(lambda row: row['VideoID']+'_'+str(int(row['Start']))+'_'+str(int(row['End']))+'.avi.npy', axis=1)\n",
    "    video_data['video_path'] = video_data['video_path'].map(lambda x: os.path.join(video_feat_path, x))\n",
    "    video_data = video_data[video_data['video_path'].map(lambda x: os.path.exists( x ))]\n",
    "    video_data = video_data[video_data['Description'].map(lambda x: isinstance(x, str))]\n",
    "\n",
    "    unique_filenames = sorted(video_data['video_path'].unique())\n",
    "    train_data = video_data[video_data['video_path'].map(lambda x: x in unique_filenames)]\n",
    "    return train_data\n",
    "\n",
    "def get_video_test_data(video_data_path, video_feat_path):\n",
    "    video_data = pd.read_csv(video_data_path, sep=',')\n",
    "    video_data = video_data[video_data['Language'] == 'English']\n",
    "    video_data['video_path'] = video_data.apply(lambda row: row['VideoID']+'_'+str(int(row['Start']))+'_'+str(int(row['End']))+'.avi.npy', axis=1)\n",
    "    video_data['video_path'] = video_data['video_path'].map(lambda x: os.path.join(video_feat_path, x))\n",
    "    video_data = video_data[video_data['video_path'].map(lambda x: os.path.exists( x ))]\n",
    "    video_data = video_data[video_data['Description'].map(lambda x: isinstance(x, str))]\n",
    "\n",
    "    unique_filenames = sorted(video_data['video_path'].unique())\n",
    "    test_data = video_data[video_data['video_path'].map(lambda x: x in unique_filenames)]\n",
    "    return test_data\n",
    "\n",
    "def preProBuildWordVocab(sentence_iterator, word_count_threshold=5):\n",
    "    # borrowed this function from NeuralTalk\n",
    "    print('preprocessing word counts and creating vocab based on word count threshold ', (word_count_threshold))\n",
    "    word_counts = {}\n",
    "    nsents = 0\n",
    "    for sent in sentence_iterator:\n",
    "        nsents += 1\n",
    "        for w in sent.lower().split(' '):\n",
    "            word_counts[w] = word_counts.get(w, 0) + 1\n",
    "    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "    print('filtered words from', (len(word_counts), ' to ', len(vocab)))\n",
    "\n",
    "    ixtoword = {}\n",
    "    ixtoword[0] = '<pad>'\n",
    "    ixtoword[1] = '<bos>'\n",
    "    ixtoword[2] = '<eos>'\n",
    "    ixtoword[3] = '<unk>'\n",
    "\n",
    "    wordtoix = {}\n",
    "    wordtoix['<pad>'] = 0\n",
    "    wordtoix['<bos>'] = 1\n",
    "    wordtoix['<eos>'] = 2\n",
    "    wordtoix['<unk>'] = 3\n",
    "\n",
    "    for idx, w in enumerate(vocab):\n",
    "        wordtoix[w] = idx+4\n",
    "        ixtoword[idx+4] = w\n",
    "\n",
    "    word_counts['<pad>'] = nsents\n",
    "    word_counts['<bos>'] = nsents\n",
    "    word_counts['<eos>'] = nsents\n",
    "    word_counts['<unk>'] = nsents\n",
    "\n",
    "    bias_init_vector = np.array([1.0 * word_counts[ ixtoword[i] ] for i in ixtoword])\n",
    "    bias_init_vector /= np.sum(bias_init_vector) # normalize to frequencies\n",
    "    bias_init_vector = np.log(bias_init_vector)\n",
    "    bias_init_vector -= np.max(bias_init_vector) # shift to nice numeric range\n",
    "\n",
    "    return wordtoix, ixtoword, bias_init_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Video_Caption_Generator():\n",
    "    def __init__(self, dim_image, n_words, dim_hidden, batch_size, n_lstm_steps, n_video_lstm_step, n_caption_lstm_step, bias_init_vector=None):\n",
    "        self.dim_image = dim_image\n",
    "        self.n_words = n_words\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.batch_size = batch_size\n",
    "        self.n_lstm_steps = n_lstm_steps\n",
    "        self.n_video_lstm_step=n_video_lstm_step\n",
    "        self.n_caption_lstm_step=n_caption_lstm_step\n",
    "\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self.Wemb = tf.Variable(tf.random_uniform([n_words, dim_hidden], -0.1, 0.1), name='Wemb')\n",
    "        #self.bemb = tf.Variable(tf.zeros([dim_hidden]), name='bemb')\n",
    "\n",
    "        #self.lstm1 = tf.nn.rnn_cell.BasicLSTMCell(dim_hidden, state_is_tuple=False)\n",
    "        #self.lstm2 = tf.nn.rnn_cell.BasicLSTMCell(dim_hidden, state_is_tuple=False)\n",
    "\n",
    "        self.lstm1 = tf.contrib.rnn.BasicLSTMCell(dim_hidden, state_is_tuple=False)\n",
    "        self.lstm2 = tf.contrib.rnn.BasicLSTMCell(dim_hidden, state_is_tuple=False)\n",
    "\n",
    "        self.encode_image_W = tf.Variable( tf.random_uniform([dim_image, dim_hidden], -0.1, 0.1), name='encode_image_W')\n",
    "        self.encode_image_b = tf.Variable( tf.zeros([dim_hidden]), name='encode_image_b')\n",
    "\n",
    "        self.embed_word_W = tf.Variable(tf.random_uniform([dim_hidden, n_words], -0.1,0.1), name='embed_word_W')\n",
    "        if bias_init_vector is not None:\n",
    "            self.embed_word_b = tf.Variable(bias_init_vector.astype(np.float32), name='embed_word_b')\n",
    "        else:\n",
    "            self.embed_word_b = tf.Variable(tf.zeros([n_words]), name='embed_word_b')\n",
    "\n",
    "    def build_model(self):\n",
    "        video = tf.placeholder(tf.float32, [self.batch_size, self.n_video_lstm_step, self.dim_image])\n",
    "        video_mask = tf.placeholder(tf.float32, [self.batch_size, self.n_video_lstm_step])\n",
    "\n",
    "        caption = tf.placeholder(tf.int32, [self.batch_size, self.n_caption_lstm_step+1])\n",
    "        caption_mask = tf.placeholder(tf.float32, [self.batch_size, self.n_caption_lstm_step+1])\n",
    "\n",
    "        video_flat = tf.reshape(video, [-1, self.dim_image])\n",
    "        image_emb = tf.nn.xw_plus_b( video_flat, self.encode_image_W, self.encode_image_b ) # (batch_size*n_lstm_steps, dim_hidden)\n",
    "        image_emb = tf.reshape(image_emb, [self.batch_size, self.n_lstm_steps, self.dim_hidden])\n",
    "\n",
    "        state1 = tf.zeros([self.batch_size, self.lstm1.state_size])\n",
    "        state2 = tf.zeros([self.batch_size, self.lstm2.state_size])\n",
    "        padding = tf.zeros([self.batch_size, self.dim_hidden])\n",
    "\n",
    "        probs = []\n",
    "        loss = 0.0\n",
    "\n",
    "        ##############################  Encoding Stage ##################################\n",
    "        for i in range(0, self.n_video_lstm_step):\n",
    "            if i > 0:\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "                output1, state1 = self.lstm1(image_emb[:,i,:], state1)\n",
    "\n",
    "            with tf.variable_scope(\"LSTM2\"):\n",
    "                output2, state2 = self.lstm2(tf.concat([padding, output1], 1), state2)\n",
    "\n",
    "        ############################# Decoding Stage ######################################\n",
    "        for i in range(0, self.n_caption_lstm_step): ## Phase 2 => only generate captions\n",
    "            #if i == 0:\n",
    "            #    current_embed = tf.zeros([self.batch_size, self.dim_hidden])\n",
    "            #else:\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                current_embed = tf.nn.embedding_lookup(self.Wemb, caption[:, i])\n",
    "\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "                output1, state1 = self.lstm1(padding, state1)\n",
    "\n",
    "            with tf.variable_scope(\"LSTM2\"):\n",
    "                output2, state2 = self.lstm2(tf.concat([current_embed, output1], 1), state2)\n",
    "\n",
    "            labels = tf.expand_dims(caption[:, i+1], 1)\n",
    "            indices = tf.expand_dims(tf.range(0, self.batch_size, 1), 1)\n",
    "            concated = tf.concat([indices, labels], 1)\n",
    "            onehot_labels = tf.sparse_to_dense(concated, tf.stack([self.batch_size, self.n_words]), 1.0, 0.0)\n",
    "\n",
    "            logit_words = tf.nn.xw_plus_b(output2, self.embed_word_W, self.embed_word_b)\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit_words, labels=onehot_labels)\n",
    "            cross_entropy = cross_entropy * caption_mask[:,i]\n",
    "            probs.append(logit_words)\n",
    "\n",
    "            current_loss = tf.reduce_sum(cross_entropy)/self.batch_size\n",
    "            loss = loss + current_loss\n",
    "\n",
    "        return loss, video, video_mask, caption, caption_mask, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train():\n",
    "    \n",
    "#if True:\n",
    "    # train_data.shape: (59590, 9), train_captions.shape: (59590,)\n",
    "    train_data = get_video_train_data(video_train_data_path, video_train_feat_path)\n",
    "    train_captions = train_data['Description'].values\n",
    "    # test_data.shape: (4051, 9), test_captions.shape: (4051,)\n",
    "    test_data = get_video_test_data(video_test_data_path, video_test_feat_path)\n",
    "    test_captions = test_data['Description'].values\n",
    "\n",
    "    # len(captions_list) = 63641\n",
    "    captions_list = list(train_captions) + list(test_captions)\n",
    "    captions = np.asarray(captions_list, dtype=np.object)\n",
    "\n",
    "    #captions = map(lambda x: x.replace('.', ''), captions)\n",
    "    #captions = map(lambda x: x.replace(',', ''), captions)\n",
    "    #captions = map(lambda x: x.replace('\"', ''), captions)\n",
    "    #captions = map(lambda x: x.replace('\\n', ''), captions)\n",
    "    #captions = map(lambda x: x.replace('?', ''), captions)\n",
    "    #captions = map(lambda x: x.replace('!', ''), captions)\n",
    "    #captions = map(lambda x: x.replace('\\\\', ''), captions)\n",
    "    #captions = map(lambda x: x.replace('/', ''), captions)\n",
    "    captions = [re.sub('[\\\",.;?!%”“]\\n\\\\/', '', x.lower()) for x in captions]\n",
    "    \n",
    "\n",
    "    # len(wordtoix): 11720, len(ixtoword): 11720, bias_init_vector.shape: (11720,)\n",
    "    wordtoix, ixtoword, bias_init_vector = preProBuildWordVocab(captions, word_count_threshold=0)\n",
    "    np.save(\"./data/wordtoix\", wordtoix)\n",
    "    np.save('./data/ixtoword', ixtoword)\n",
    "    np.save(\"./data/bias_init_vector\", bias_init_vector)\n",
    "\n",
    "    model = Video_Caption_Generator(dim_image=dim_image,                     # 4096\n",
    "                                    n_words=len(wordtoix),                   # 11720\n",
    "                                    dim_hidden=dim_hidden,                   # 1000\n",
    "                                    batch_size=batch_size,                   # 50\n",
    "                                    n_lstm_steps=n_frame_step,               # 80\n",
    "                                    n_video_lstm_step=n_video_lstm_step,     # 80\n",
    "                                    n_caption_lstm_step=n_caption_lstm_step, # 20\n",
    "                                    bias_init_vector=bias_init_vector)       # bias_init_vector.shape: (11720,)\n",
    "    \n",
    "    tf_loss, tf_video, tf_video_mask, tf_caption, tf_caption_mask, tf_probs = model.build_model()\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    # my tensorflow version is 0.12.1, I write the saver with version 1.0\n",
    "    saver = tf.train.Saver(max_to_keep=100, write_version=1)\n",
    "    with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE) as scope:\n",
    "        #assert tf.get_variable_scope().reuse == True \n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(tf_loss)\n",
    "    tf.global_variables_initializer().run()\n",
    "    loss_fd = open('loss.txt', 'w')\n",
    "    loss_to_draw = []\n",
    "    \n",
    "    #################################\n",
    "    for epoch in range(0, n_epochs):\n",
    "        loss_to_draw_epoch = []\n",
    "\n",
    "        index = list(train_data.index)\n",
    "        np.random.shuffle(index)\n",
    "        train_data = train_data.ix[index]\n",
    "\n",
    "        current_train_data = train_data.groupby('video_path').apply(lambda x: x.iloc[np.random.choice(len(x))])\n",
    "        current_train_data = current_train_data.reset_index(drop=True)\n",
    "\n",
    "        for start, end in zip(\n",
    "                range(0, len(current_train_data), batch_size),\n",
    "                range(batch_size, len(current_train_data), batch_size)):\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            current_batch = current_train_data[start:end]\n",
    "            current_videos = current_batch['video_path'].values\n",
    "\n",
    "            current_feats = np.zeros((batch_size, n_video_lstm_step, dim_image))\n",
    "            #current_feats_vals = map(lambda vid: np.load(vid), current_videos)\n",
    "            current_feats_vals = [np.load(vid) for vid in current_videos]\n",
    "\n",
    "            current_video_masks = np.zeros((batch_size, n_video_lstm_step))\n",
    "\n",
    "            for ind,feat in enumerate(current_feats_vals):\n",
    "                \n",
    "                current_feats[ind][:len(list(current_feats_vals)[ind])] = feat\n",
    "                current_video_masks[ind][:len(list(current_feats_vals)[ind])] = 1\n",
    "\n",
    "            current_captions = current_batch['Description'].values\n",
    "            #current_captions = map(lambda x: '<bos> ' + x, current_captions)\n",
    "            #current_captions = map(lambda x: x.replace('.', ''), current_captions)\n",
    "            #current_captions = map(lambda x: x.replace(',', ''), current_captions)\n",
    "            #current_captions = map(lambda x: x.replace('\"', ''), current_captions)\n",
    "            #current_captions = map(lambda x: x.replace('\\n', ''), current_captions)\n",
    "            #current_captions = map(lambda x: x.replace('?', ''), current_captions)\n",
    "            #current_captions = map(lambda x: x.replace('!', ''), current_captions)\n",
    "            #current_captions = map(lambda x: x.replace('\\\\', ''), current_captions)\n",
    "            #current_captions = map(lambda x: x.replace('/', ''), current_captions)\n",
    " \n",
    "            current_captions = ['<bos> ' + x for x in current_captions]\n",
    "            current_captions = [re.sub('[\\\",.;?!%”“]\\n\\\\/', '', x.lower()) for x in current_captions]\n",
    "            current_captions = [x.replace(' 2 ', ' two ') for x in current_captions]\n",
    "            current_captions = [x.replace(' 3 ', ' three ') for x in current_captions]\n",
    "            current_captions = [x.replace(' 4 ', ' four ') for x in current_captions]\n",
    "            current_captions = [x.replace(' 5 ', ' five ') for x in current_captions]\n",
    "            current_captions = [x.replace(' 6 ', ' six ') for x in current_captions]\n",
    "\n",
    "\n",
    "            for idx, each_cap in enumerate(current_captions):\n",
    "                word = each_cap.lower().split(' ')\n",
    "                if len(word) < n_caption_lstm_step:\n",
    "                    current_captions[idx] = current_captions[idx] + ' <eos>'\n",
    "                else:\n",
    "                    new_word = ''\n",
    "                    for i in range(n_caption_lstm_step-1):\n",
    "                        new_word = new_word + word[i] + ' '\n",
    "                    current_captions[idx] = new_word + '<eos>'\n",
    "\n",
    "            current_caption_ind = []\n",
    "            for cap in current_captions:\n",
    "                current_word_ind = []\n",
    "                for word in cap.lower().split(' '):\n",
    "                    if word in wordtoix:\n",
    "                        current_word_ind.append(wordtoix[word])\n",
    "                    else:\n",
    "                        current_word_ind.append(wordtoix['<unk>'])\n",
    "                current_caption_ind.append(current_word_ind)\n",
    "\n",
    "            current_caption_matrix = sequence.pad_sequences(current_caption_ind, padding='post', maxlen=n_caption_lstm_step)\n",
    "            current_caption_matrix = np.hstack( [current_caption_matrix, np.zeros( [len(current_caption_matrix), 1] ) ] ).astype(int)\n",
    "            current_caption_masks = np.zeros( (current_caption_matrix.shape[0], current_caption_matrix.shape[1]) )\n",
    "            #nonzeros = np.array( map(lambda x: (x != 0).sum() + 1, current_caption_matrix ) )\n",
    "            nonzeros = np.array( [(x != 0).sum() + 1 for x in current_caption_matrix ])\n",
    "                                \n",
    "            for ind, row in enumerate(current_caption_masks):\n",
    "                row[:nonzeros[ind]] = 1\n",
    "\n",
    "            probs_val = sess.run(tf_probs, feed_dict={\n",
    "                tf_video:current_feats,\n",
    "                tf_caption: current_caption_matrix\n",
    "                })\n",
    "\n",
    "            _, loss_val = sess.run(\n",
    "                    [train_op, tf_loss],\n",
    "                    feed_dict={\n",
    "                        tf_video: current_feats,\n",
    "                        tf_video_mask : current_video_masks,\n",
    "                        tf_caption: current_caption_matrix,\n",
    "                        tf_caption_mask: current_caption_masks\n",
    "                        })\n",
    "            loss_to_draw_epoch.append(loss_val)\n",
    "\n",
    "            print('idx: ', start, \" Epoch: \", epoch, \" loss: \", loss_val, ' Elapsed time: ', str((time.time() - start_time)))\n",
    "            loss_fd.write('epoch ' + str(epoch) + ' loss ' + str(loss_val) + '\\n')\n",
    "\n",
    "        # draw loss curve every epoch\n",
    "        loss_to_draw.append(np.mean(loss_to_draw_epoch))\n",
    "        plt_save_dir = \"./loss_imgs\"\n",
    "        plt_save_img_name = str(epoch) + '.png'\n",
    "        plt.plot(range(len(loss_to_draw)), loss_to_draw, color='g')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(plt_save_dir, plt_save_img_name))\n",
    "\n",
    "        if np.mod(epoch, 10) == 0:\n",
    "            print(\"Epoch \", epoch, \" is done. Saving the model ...\")\n",
    "            saver.save(sess, os.path.join(model_path, 'model'), global_step=epoch)\n",
    "    #################################\n",
    "    loss_fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(model_path='./models/model-100'):\n",
    "    test_data = get_video_test_data(video_test_data_path, video_test_feat_path)\n",
    "    #test_videos = test_data['video_path'].unique()\n",
    "    test_videos = np.array(['./rgb_test_features/klteYv1Uv9A_27_33.avi.npy', \n",
    "                        './rgb_test_features/5YJaS2Eswg0_22_26.avi.npy',\n",
    "                        './rgb_test_features/UbmZAe5u5FI_132_141.avi.npy',\n",
    "                        './rgb_test_features/JntMAcTlOF0_50_70.avi.npy',\n",
    "                        './rgb_test_features/tJHUH9tpqPg_113_118.avi.npy',])\n",
    "\n",
    "    ixtoword = pd.Series(np.load('./data/ixtoword.npy').tolist())\n",
    "\n",
    "    bias_init_vector = np.load('./data/bias_init_vector.npy')\n",
    "\n",
    "    model = Video_Caption_Generator(\n",
    "            dim_image=dim_image,\n",
    "            n_words=len(ixtoword),\n",
    "            dim_hidden=dim_hidden,\n",
    "            batch_size=batch_size,\n",
    "            n_lstm_steps=n_frame_step,\n",
    "            n_video_lstm_step=n_video_lstm_step,\n",
    "            n_caption_lstm_step=n_caption_lstm_step,\n",
    "            bias_init_vector=bias_init_vector)\n",
    "\n",
    "    video_tf, video_mask_tf, caption_tf, probs_tf, last_embed_tf = model.build_generator()\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, model_path)\n",
    "\n",
    "    test_output_txt_fd = open('S2VT_results.txt', 'w')\n",
    "    for idx, video_feat_path in enumerate(test_videos):\n",
    "        print(idx, video_feat_path)\n",
    "\n",
    "        video_feat = np.load(video_feat_path)[None,...]\n",
    "        #video_feat = np.load(video_feat_path)\n",
    "        #video_mask = np.ones((video_feat.shape[0], video_feat.shape[1]))\n",
    "        if video_feat.shape[1] == n_frame_step:\n",
    "            video_mask = np.ones((video_feat.shape[0], video_feat.shape[1]))\n",
    "        else:\n",
    "            continue\n",
    "            #shape_templete = np.zeros(shape=(1, n_frame_step, 4096), dtype=float )\n",
    "            #shape_templete[:video_feat.shape[0], :video_feat.shape[1], :video_feat.shape[2]] = video_feat\n",
    "            #video_feat = shape_templete\n",
    "            #video_mask = np.ones((video_feat.shape[0], n_frame_step))\n",
    "\n",
    "        generated_word_index = sess.run(caption_tf, feed_dict={video_tf:video_feat, video_mask_tf:video_mask})\n",
    "        generated_words = ixtoword[generated_word_index]\n",
    "\n",
    "        punctuation = np.argmax(np.array(generated_words) == '<eos>') + 1\n",
    "        generated_words = generated_words[:punctuation]\n",
    "\n",
    "        generated_sentence = ' '.join(generated_words)\n",
    "        generated_sentence = generated_sentence.replace('<bos> ', '')\n",
    "        generated_sentence = generated_sentence.replace(' <eos>', '')\n",
    "        print(generated_sentence,'\\n')\n",
    "        test_output_txt_fd.write(video_feat_path + '\\n')\n",
    "        test_output_txt_fd.write(generated_sentence + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing word counts and creating vocab based on word count threshold  0\n",
      "filtered words from (15351, ' to ', 15351)\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fabf02a1160>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fabf02a10f0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/ipykernel_launcher.py:61: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[50,80,256]\n\t [[Node: gradients_7/LSTM1_706/strided_slice_grad/StridedSliceGrad = StridedSliceGrad[Index=DT_INT32, T=DT_FLOAT, begin_mask=5, ellipsis_mask=0, end_mask=5, new_axis_mask=0, shrink_axis_mask=2, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients_7/LSTM1_706/strided_slice_grad/Shape, LSTM1_706/strided_slice/stack, LSTM1_706/strided_slice/stack_1, LSTM1_706/strided_slice/stack_2, gradients_7/LSTM1_700/LSTM1/basic_lstm_cell/concat_12_grad/tuple/control_dependency)]]\n\nCaused by op 'gradients_7/LSTM1_706/strided_slice_grad/StridedSliceGrad', defined at:\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-42-8eb72a3b23a1>\", line 2, in <module>\n    train()\n  File \"<ipython-input-41-8fbd31500797>\", line 50, in train\n    train_op = tf.train.AdamOptimizer(learning_rate).minimize(tf_loss)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 343, in minimize\n    grad_loss=grad_loss)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 414, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 353, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py\", line 245, in _StridedSliceGrad\n    shrink_axis_mask=op.get_attr(\"shrink_axis_mask\")), None, None, None\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 5572, in strided_slice_grad\n    shrink_axis_mask=shrink_axis_mask, name=name)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'LSTM1_706/strided_slice', defined at:\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 19 identical lines from previous traceback]\n  File \"<ipython-input-42-8eb72a3b23a1>\", line 2, in <module>\n    train()\n  File \"<ipython-input-41-8fbd31500797>\", line 43, in train\n    tf_loss, tf_video, tf_video_mask, tf_caption, tf_caption_mask, tf_probs = model.build_model()\n  File \"<ipython-input-2-f5efa2e25a88>\", line 54, in build_model\n    output1, state1 = self.lstm1(image_emb[:,i,:], state1)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 538, in _SliceHelper\n    name=name)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 706, in strided_slice\n    shrink_axis_mask=shrink_axis_mask)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 5430, in strided_slice\n    name=name)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[50,80,256]\n\t [[Node: gradients_7/LSTM1_706/strided_slice_grad/StridedSliceGrad = StridedSliceGrad[Index=DT_INT32, T=DT_FLOAT, begin_mask=5, ellipsis_mask=0, end_mask=5, new_axis_mask=0, shrink_axis_mask=2, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients_7/LSTM1_706/strided_slice_grad/Shape, LSTM1_706/strided_slice/stack, LSTM1_706/strided_slice/stack_1, LSTM1_706/strided_slice/stack_2, gradients_7/LSTM1_700/LSTM1/basic_lstm_cell/concat_12_grad/tuple/control_dependency)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[50,80,256]\n\t [[Node: gradients_7/LSTM1_706/strided_slice_grad/StridedSliceGrad = StridedSliceGrad[Index=DT_INT32, T=DT_FLOAT, begin_mask=5, ellipsis_mask=0, end_mask=5, new_axis_mask=0, shrink_axis_mask=2, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients_7/LSTM1_706/strided_slice_grad/Shape, LSTM1_706/strided_slice/stack, LSTM1_706/strided_slice/stack_1, LSTM1_706/strided_slice/stack_2, gradients_7/LSTM1_700/LSTM1/basic_lstm_cell/concat_12_grad/tuple/control_dependency)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-8eb72a3b23a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hour'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m3600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m3600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-8fbd31500797>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m                         \u001b[0mtf_video_mask\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mcurrent_video_masks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                         \u001b[0mtf_caption\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcurrent_caption_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                         \u001b[0mtf_caption_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcurrent_caption_masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m                         })\n\u001b[1;32m    148\u001b[0m             \u001b[0mloss_to_draw_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[50,80,256]\n\t [[Node: gradients_7/LSTM1_706/strided_slice_grad/StridedSliceGrad = StridedSliceGrad[Index=DT_INT32, T=DT_FLOAT, begin_mask=5, ellipsis_mask=0, end_mask=5, new_axis_mask=0, shrink_axis_mask=2, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients_7/LSTM1_706/strided_slice_grad/Shape, LSTM1_706/strided_slice/stack, LSTM1_706/strided_slice/stack_1, LSTM1_706/strided_slice/stack_2, gradients_7/LSTM1_700/LSTM1/basic_lstm_cell/concat_12_grad/tuple/control_dependency)]]\n\nCaused by op 'gradients_7/LSTM1_706/strided_slice_grad/StridedSliceGrad', defined at:\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-42-8eb72a3b23a1>\", line 2, in <module>\n    train()\n  File \"<ipython-input-41-8fbd31500797>\", line 50, in train\n    train_op = tf.train.AdamOptimizer(learning_rate).minimize(tf_loss)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 343, in minimize\n    grad_loss=grad_loss)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 414, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 353, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py\", line 245, in _StridedSliceGrad\n    shrink_axis_mask=op.get_attr(\"shrink_axis_mask\")), None, None, None\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 5572, in strided_slice_grad\n    shrink_axis_mask=shrink_axis_mask, name=name)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'LSTM1_706/strided_slice', defined at:\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 19 identical lines from previous traceback]\n  File \"<ipython-input-42-8eb72a3b23a1>\", line 2, in <module>\n    train()\n  File \"<ipython-input-41-8fbd31500797>\", line 43, in train\n    tf_loss, tf_video, tf_video_mask, tf_caption, tf_caption_mask, tf_probs = model.build_model()\n  File \"<ipython-input-2-f5efa2e25a88>\", line 54, in build_model\n    output1, state1 = self.lstm1(image_emb[:,i,:], state1)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 538, in _SliceHelper\n    name=name)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 706, in strided_slice\n    shrink_axis_mask=shrink_axis_mask)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 5430, in strided_slice\n    name=name)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/bonzo/anaconda3/envs/keras2.0/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[50,80,256]\n\t [[Node: gradients_7/LSTM1_706/strided_slice_grad/StridedSliceGrad = StridedSliceGrad[Index=DT_INT32, T=DT_FLOAT, begin_mask=5, ellipsis_mask=0, end_mask=5, new_axis_mask=0, shrink_axis_mask=2, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients_7/LSTM1_706/strided_slice_grad/Shape, LSTM1_706/strided_slice/stack, LSTM1_706/strided_slice/stack_1, LSTM1_706/strided_slice/stack_2, gradients_7/LSTM1_700/LSTM1/basic_lstm_cell/concat_12_grad/tuple/control_dependency)]]\n"
     ]
    }
   ],
   "source": [
    "sta = time.time()\n",
    "train()\n",
    "end = time.time()\n",
    "t = end - sta\n",
    "print('time:', t/3600, 'hour', (t%3600)/60, 'min', (t%3600)%60, 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-044a1eae16ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcurrent_video_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_feats_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "current_video_masks[ind][:len(list(current_feats_vals)[ind])] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-808d259bcf26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_feats_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "len(list(current_feats_vals)[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(current_feats_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_feats_vals = map(lambda vid: np.load(vid), current_videos)\n",
    "#current_feats_vals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x7fac25e0aac8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(lambda vid: np.load(vid), current_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos> A person stabs a photo print with a knife',\n",
       " '<bos> okra is cooking in boiling water',\n",
       " '<bos> a boy playing music',\n",
       " '<bos> A female swimmer pushes off the side of pool underwater',\n",
       " '<bos> A baby husky is in a cage with a larger husky that is walking around and sniffing it',\n",
       " '<bos> A woman plays a flute',\n",
       " '<bos> A young woman is exercising on a mat',\n",
       " '<bos> the woman is cooking',\n",
       " '<bos> A woman slices a zucchini',\n",
       " '<bos> A man cuts a cucumber with skin into two and thereafter slices it finely',\n",
       " '<bos> A man is cutting a cucumber',\n",
       " '<bos> A woman pours a beaten egg mixture into a hot pan and swirls the pan to spread it evenly',\n",
       " '<bos> A person takes meat and vegetables out of a pot',\n",
       " '<bos> the women is cooking something',\n",
       " '<bos> The lady made the sushi look like an octopus',\n",
       " '<bos> A woman is slicing some Bentos',\n",
       " '<bos> A carrot is chopped up',\n",
       " '<bos> the cooking with dog',\n",
       " '<bos> A woman mixes a beauty treatment in a glass',\n",
       " '<bos> People are running in a race',\n",
       " '<bos> Her tutorial is very niceand creativeI liked the makeup',\n",
       " '<bos> A train is coming down the tracks',\n",
       " '<bos> The panda climbed up the tower in his pen',\n",
       " '<bos> the girl is cutting the laeves',\n",
       " '<bos> A WOMAN HOLD POT OF WATERING PLANT',\n",
       " '<bos> Someone is cutting a potatoe',\n",
       " '<bos> a man and woman dancing',\n",
       " '<bos> A Girls singing in hotel',\n",
       " '<bos> A baby rhino is following an adult rhino',\n",
       " '<bos> Three animated women dance',\n",
       " '<bos> A kid rides his bike',\n",
       " '<bos> A person is making Yakibuta Ramen',\n",
       " '<bos> A person is peeling hard boiled eggs',\n",
       " '<bos> a man making pieces of vegitable looking like onion',\n",
       " '<bos> A woman puts leafy vegetables into a pot of water',\n",
       " '<bos> A woman cooks two big pieces of meat in a pan',\n",
       " '<bos> A woman is cutting a meat',\n",
       " '<bos> someone show how to make the japanese noodles',\n",
       " '<bos> balls is flying',\n",
       " '<bos> Someone is rubbing a frog',\n",
       " '<bos> The boar is running',\n",
       " '<bos> A person is peeling potato',\n",
       " '<bos> A man is cutting a potato in to pieces',\n",
       " '<bos> One person is explaining how to prepair potato salad',\n",
       " '<bos> A cooker is making the boiled egg into four pieces',\n",
       " '<bos> A red panda is eating food from someones hand',\n",
       " '<bos> A cat is putting its head into  a brown paper cover',\n",
       " '<bos> A man in a construction hat is dancing while two other seated men watch',\n",
       " '<bos> A woman is water skiing']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(current_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 21)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_caption_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-b2fc315194af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnonzeros\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "row[:nonzeros[ind]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-5c1f6215349a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnonzeros\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "nonzeros[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(<map object at 0x7fac0fe55080>, dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
