{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1450 training set, 100 testing set\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "data_dir = './MLDS_hw2_data/'\n",
    "test_out_dir = './test_out_dir/'\n",
    "peer_out_dir = './peer_out_dir/'\n",
    "\n",
    "train_feat_list = glob.glob(data_dir+'training_data/feat/*')\n",
    "test_feat_list = glob.glob(data_dir+'testing_data/feat/*')\n",
    "print(len(train_feat_list), 'training set,', len(test_feat_list), 'testing set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training features ...\n",
      "train_data.shape: (1450, 80, 4096) train_data.dtype: float64\n",
      "pickling train_data\n"
     ]
    }
   ],
   "source": [
    "print('loading training features ...')\n",
    "train_data = list()\n",
    "for train_feat in train_feat_list:\n",
    "    train_data.append(np.load(train_feat))\n",
    "else:\n",
    "    train_data = np.array(train_data)\n",
    "    print('train_data.shape:', train_data.shape, 'train_data.dtype:', train_data.dtype)\n",
    "    print('pickling train_data')\n",
    "    pickle.dump(train_data, open('train_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading testing features ...\n",
      "test_data.shape: (100, 80, 4096) test_data.dtype: float64\n",
      "pickling test_data\n"
     ]
    }
   ],
   "source": [
    "print('loading testing features ...')\n",
    "test_data = list()\n",
    "for test_feat in test_feat_list:\n",
    "    test_data.append(np.load(test_feat))\n",
    "else:\n",
    "    test_data = np.array(test_data)\n",
    "    print('test_data.shape:', test_data.shape, 'test_data.dtype:', test_data.dtype)\n",
    "    print('pickling test_data')\n",
    "    pickle.dump(test_data, open('test_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train_data, test_data from .pkl ...\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "print('loading train_data, test_data from .pkl ...')\n",
    "train_data = pickle.load(open('train_data.pkl', 'rb'))\n",
    "test_data = pickle.load(open('test_data.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training label ...\n",
      "loading training id ...\n",
      "loading testing label ...\n",
      "loading testing id ...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print('loading training label ...')\n",
    "train_label_json = json.load(open(data_dir+'training_label.json'))\n",
    "print('loading training id ...')\n",
    "training_id = [x.split('/')[-1].replace('.npy','') for x in train_feat_list]\n",
    "\n",
    "print('loading testing label ...')\n",
    "test_label_json = json.load(open(data_dir+'testing_label.json'))\n",
    "print('loading testing id ...')\n",
    "testing_id = [x.split('/')[-1].replace('.npy','') for x in test_feat_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 6345 words in training/testing label\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "caption_words = set()\n",
    "caption_dict = {'2':'two', '3':'three', '4':'four', '5':'five', '6':'six'}\n",
    "for tj in train_label_json + test_label_json:\n",
    "    for c in tj['caption']:\n",
    "        c = c.replace('one hundred', '100')\n",
    "        c = c.replace('two hundred', '200')\n",
    "        s = [re.sub('[\\\",.;?!%”“]', '', s.lower()) for s in c.split(' ')]\n",
    "        \n",
    "        s[-1] = s[-1].replace('.', '')\n",
    "        caption_words |= set(s)\n",
    "else:\n",
    "    caption_words.add('<BOS>')\n",
    "    caption_words.add('<EOS>')\n",
    "    caption_words.add('<NULL>')\n",
    "    \n",
    "    caption_words.remove('2')\n",
    "    caption_words.remove('3')\n",
    "    caption_words.remove('4')\n",
    "    caption_words.remove('5')\n",
    "    caption_words.remove('6')\n",
    "    caption_words.remove('')\n",
    "            \n",
    "    print('there are', len(caption_words), 'words in training/testing label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print ('Get embedding dict from glove.')\n",
    "#embedding_dict = get_embedding_dict('glove.6B.100d.txt')\n",
    "#print ('Found', len(embedding_dict), 'word vectors')\n",
    "#num_words = len(caption_words)\n",
    "#print ('Create embedding matrix.')\n",
    "#embedding_matrix = get_embedding_matrix(word_index,embedding_dict,num_words,embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caption_words label encoding...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "print('caption_words label encoding...')\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(list(caption_words))\n",
    "#print('encoding', len(le.classes_), 'classes')\n",
    "#le.transform([1, 1, 2, 6]) \n",
    "#le.inverse_transform([0, 0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#keras.utils.to_categorical(, num_classes=6345)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 hit at i: 1069 replaced by two\n",
      "2 hit at i: 1449 replaced by two\n",
      "4 hit at i: 659 replaced by four\n",
      "4 hit at i: 659 replaced by four\n",
      "3 hit at i: 25 replaced by three\n",
      "6 hit at i: 931 replaced by six\n",
      "5 hit at i: 1429 replaced by five\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "train_label_json_split = copy.deepcopy(train_label_json)\n",
    "for tid in training_id:\n",
    "    for i, tj in enumerate(train_label_json_split):\n",
    "        if tid == tj['id']:\n",
    "            caption = list()\n",
    "            caption_one_hot = list()\n",
    "            caption_mask = list()\n",
    "            for c in tj['caption']:\n",
    "                c = c.replace('one hundred', '100')\n",
    "                c = c.replace('two hundred', '200')\n",
    "                s = [re.sub('[\\\",.;?!%”“]', '', s.lower()) for s in c.split(' ') ]\n",
    "                s = [ss for ss in s if ss]\n",
    "\n",
    "                \n",
    "                # 將2, 3, 4, 5, 6用two, three, four, five, six取代\n",
    "                for si, ss in enumerate(s):\n",
    "                    if ss in caption_dict:\n",
    "                        print(ss, 'hit at i:', i, 'replaced by', caption_dict[ss])\n",
    "                        s[si] = caption_dict[ss]\n",
    "                        \n",
    "                s.insert(0, '<BOS>')\n",
    "                s.append('<EOS>')\n",
    "                for si in range(len(s), 37):\n",
    "                    s.append('<NULL>')\n",
    "                for si in range(80):\n",
    "                    s.insert(0, '<NULL>')\n",
    "                    \n",
    "                caption.append(s)\n",
    "                s_one_hot = le.transform(s)\n",
    "                caption_one_hot.append(s_one_hot)\n",
    "                caption_mask.append(s_one_hot == le.transform(['<NULL>']))\n",
    "            else:\n",
    "                train_label_json_split[i]['caption'] = caption\n",
    "                train_label_json_split[i]['one_hot'] = caption_one_hot\n",
    "                train_label_json_split[i]['mask'] = caption_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "test_label_json_split = copy.deepcopy(test_label_json)\n",
    "for tid in testing_id:\n",
    "    for i, tj in enumerate(test_label_json_split):\n",
    "        if tid == tj['id']:\n",
    "            caption = list()\n",
    "            caption_one_hot = list()\n",
    "            caption_mask = list()\n",
    "            for c in tj['caption']:\n",
    "                c = c.replace('one hundred', '100')\n",
    "                c = c.replace('two hundred', '200')\n",
    "                s = [re.sub('[\\\",.;?!%”“]', '', s.lower()) for s in c.split(' ')]\n",
    "                s = [ss for ss in s if ss]\n",
    "                \n",
    "                # 將2, 3, 4, 5, 6用two, three, four, five, six取代\n",
    "                for si, ss in enumerate(s):\n",
    "                    if ss in caption_dict:\n",
    "                        print(ss, 'hit at i:', i, 'replaced by', caption_dict[ss])\n",
    "                        s[si] = caption_dict[ss]\n",
    "                        \n",
    "                s.insert(0, '<BOS>')\n",
    "                s.append('<EOS>')\n",
    "                for si in range(len(s), 37):\n",
    "                    s.append('<NULL>')\n",
    "                for si in range(80):\n",
    "                    s.insert(0, '<NULL>')\n",
    "\n",
    "                caption.append(s)\n",
    "                s_one_hot = le.transform(s)\n",
    "                caption_one_hot.append(s_one_hot)\n",
    "                caption_mask.append(s_one_hot == le.transform(['<NULL>']))\n",
    "            else:\n",
    "                test_label_json_split[i]['caption'] = caption\n",
    "                test_label_json_split[i]['one_hot'] = caption_one_hot\n",
    "                test_label_json_split[i]['mask'] = caption_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of caption is 37\n"
     ]
    }
   ],
   "source": [
    "print('max length of caption is', 37)\n",
    "#max_ = 0\n",
    "#for t in train_label_json_split+test_label_json_split:\n",
    "#    if len(t['caption']) > max_:\n",
    "#        max_ = len(t['caption'])\n",
    "#else:\n",
    "#    print('max length of caption is', max_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding phase length: 1450  decoding phase length: 37\n",
      "pad train_data from (80, 4096) to ( 117 , 4096 )\n",
      "shape after padding: (1450, 117, 4096)\n"
     ]
    }
   ],
   "source": [
    "print('encoding phase length:', train_data.shape[0], ' decoding phase length:', 37)\n",
    "print('pad train_data from', train_data.shape[1:], 'to (', train_data.shape[1]+37, ',', train_data.shape[2],')') \n",
    "\n",
    "train_data_padding = list()\n",
    "for t in train_data:\n",
    "    train_data_padding.append(np.concatenate([t, np.zeros((37, 4096))]))\n",
    "else:\n",
    "    train_data_padding = np.array(train_data_padding)\n",
    "    print('shape after padding:', train_data_padding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_padding.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding phase length: 100  decoding phase length: 37\n",
      "pad test_data from (80, 4096) to ( 117 , 4096 )\n",
      "shape after padding: (100, 117, 4096)\n"
     ]
    }
   ],
   "source": [
    "print('encoding phase length:', test_data.shape[0], ' decoding phase length:', 37)\n",
    "print('pad test_data from', test_data.shape[1:], 'to (', test_data.shape[1]+37, ',', test_data.shape[2],')') \n",
    "\n",
    "test_data_padding = list()\n",
    "for t in test_data:\n",
    "    test_data_padding.append(np.concatenate([t, np.zeros((37, 4096))]))\n",
    "else:\n",
    "    test_data_padding = np.array(test_data_padding)\n",
    "    print('shape after padding:', test_data_padding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_label_padding.shape: (1450, 117, 6345) train_label_mask.shape: (1450, 117, 1)\n",
      "test_label_padding.shape: (100, 117, 6345) test_label_mask.shape: (100, 117, 1)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_label_padding = list()\n",
    "train_label_mask = list()\n",
    "\n",
    "test_label_padding = list()\n",
    "test_label_mask = list()\n",
    "def random_pick_gen_label():\n",
    "    global train_label_padding\n",
    "    global train_label_mask\n",
    "    global test_label_padding\n",
    "    global test_label_mask\n",
    "#if True:    \n",
    "    for i, tid in enumerate(training_id):\n",
    "        for tj in train_label_json_split:\n",
    "            if tid == tj['id']:\n",
    "                k = random.randint(0, len(tj['one_hot'])-1)\n",
    "                                \n",
    "                train_label_padding.append(to_categorical(tj['one_hot'][k], num_classes=6345))\n",
    "                train_label_mask.append(tj['mask'][k].reshape((117,1)))\n",
    "    else:\n",
    "        train_label_padding = np.array(train_label_padding)\n",
    "        train_label_padding = train_label_padding.reshape((1450, 117, 6345))\n",
    "        train_label_mask = np.array(train_label_mask)\n",
    "        train_label_mask = train_label_mask.reshape((1450, 117, 1))\n",
    "        \n",
    "                \n",
    "\n",
    "    for i, tid in enumerate(testing_id):\n",
    "        for tj in test_label_json_split:\n",
    "            if tid == tj['id']:\n",
    "                k = random.randint(0, len(tj['one_hot'])-1)\n",
    "                                \n",
    "                test_label_padding.append(to_categorical(tj['one_hot'][k], num_classes=6345))\n",
    "                test_label_mask.append(tj['mask'][k].reshape((117,1)))\n",
    "    else:\n",
    "        test_label_padding = np.array(test_label_padding)\n",
    "        test_label_padding = test_label_padding.reshape((100, 117, 6345))\n",
    "        test_label_mask = np.array(test_label_mask)\n",
    "        test_label_mask = test_label_mask.reshape((100, 117, 1))\n",
    "        \n",
    "\n",
    "random_pick_gen_label()\n",
    "print('train_label_padding.shape:', train_label_padding.shape, 'train_label_mask.shape:', train_label_mask.shape)\n",
    "print('test_label_padding.shape:', test_label_padding.shape, 'test_label_mask.shape:', test_label_mask.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import keras.backend as K \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.layers import GRU\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# Headline input: meant to receive sequences of 100 integers, between 1 and 10000.\n",
    "# Note that we can name any layer by passing it a \"name\" argument.\n",
    "#main_input = Input(shape=(,117, 4096), dtype='float32', name='main_input')\n",
    "video_input = Input(shape=(None, 117, 4096), dtype='float32', name='video_input')\n",
    "\n",
    "# This embedding layer will encode the input sequence\n",
    "# into a sequence of dense 512-dimensional vectors.\n",
    "#x = Embedding(output_dim=512, input_dim=10000, input_length=100)(main_input)\n",
    "\n",
    "# A LSTM will transform the vector sequence into a single vector,\n",
    "# containing information about the entire sequence\n",
    "encoder_out = LSTM(256, return_sequences=True, dropout=0.5)(video_input)\n",
    "\n",
    "#auxiliary_output = Dense(1, activation='sigmoid', name='aux_output')(lstm_out)\n",
    "\n",
    "caption_input = Input(shape=(None, 117, 6354), name='caption_input')\n",
    "x = keras.layers.concatenate([encoder_out, caption_input])\n",
    "x = LSTM(256, return_sequences=True, dropout=0.5)(x)\n",
    "# We stack a deep densely-connected network on top\n",
    "x = Dense(6354, activation='softmax', name='output')(x)\n",
    "model = Model(inputs=[video_input, caption_input], outputs=[output])\n",
    "\n",
    "\n",
    "#model.compile(optimizer='adam', loss='myloss',\n",
    "#              loss_weights=[1., 0.2])\n",
    "\n",
    "#model.fit([headline_data, additional_data], [labels, labels],\n",
    "#          epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = Sequential()\n",
    "#model.add(LSTM(256, return_sequences=True, activation='sigmoid', dropout=0.25), input_shape=(sample_length, dim))\n",
    "#model.add(LSTM(256, return_sequences=True, activation='sigmoid', dropout=0.25), input_shape=(sample_length, dim))\n",
    "                     \n",
    "#            model.add(Dense(256,activation='sigmoid'))\n",
    "#            model.add(Dropout(0.25))\n",
    "#            model.add(Dense(64,activation='sigmoid'))\n",
    "#            model.add(Dropout(0.25))\n",
    "#            model.add(Dense(48,activation='softmax'))\n",
    "#            model.summary()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
