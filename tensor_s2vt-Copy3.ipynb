{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/twilitspiderman/anaconda3/envs/keras2.0/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import re\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "video_train_feat_path = './rgb_train_features'\n",
    "video_train_data_path = './data/video_corpus.csv'\n",
    "video_test_feat_path = './rgb_test_features'\n",
    "video_test_data_path = './data/video_corpus.csv'\n",
    "model_path = './models'\n",
    "\n",
    "dim_image = 4096\n",
    "dim_hidden= 128\n",
    "\n",
    "n_video_lstm_step = 80\n",
    "n_caption_lstm_step = 40\n",
    "n_frame_step = 80\n",
    "\n",
    "n_epochs = 200\n",
    "#n_epochs = 2\n",
    "\n",
    "batch_size = 50\n",
    "learning_rate = 0.0001\n",
    "\"\"\"\n",
    "def get_video_train_data(video_data_path, video_feat_path):\n",
    "    video_data = pd.read_csv(video_data_path, sep=',')\n",
    "    video_data = video_data[video_data['Language'] == 'English']\n",
    "    video_data['video_path'] = video_data.apply(lambda row: row['VideoID']+'_'+str(int(row['Start']))+'_'+str(int(row['End']))+'.avi.npy', axis=1)\n",
    "    video_data['video_path'] = video_data['video_path'].map(lambda x: os.path.join(video_feat_path, x))\n",
    "    video_data = video_data[video_data['video_path'].map(lambda x: os.path.exists( x ))]\n",
    "    video_data = video_data[video_data['Description'].map(lambda x: isinstance(x, str))]\n",
    "\n",
    "    unique_filenames = sorted(video_data['video_path'].unique())\n",
    "    train_data = video_data[video_data['video_path'].map(lambda x: x in unique_filenames)]\n",
    "    return train_data\n",
    "\n",
    "def get_video_test_data(video_data_path, video_feat_path):\n",
    "    video_data = pd.read_csv(video_data_path, sep=',')\n",
    "    video_data = video_data[video_data['Language'] == 'English']\n",
    "    video_data['video_path'] = video_data.apply(lambda row: row['VideoID']+'_'+str(int(row['Start']))+'_'+str(int(row['End']))+'.avi.npy', axis=1)\n",
    "    video_data['video_path'] = video_data['video_path'].map(lambda x: os.path.join(video_feat_path, x))\n",
    "    video_data = video_data[video_data['video_path'].map(lambda x: os.path.exists( x ))]\n",
    "    video_data = video_data[video_data['Description'].map(lambda x: isinstance(x, str))]\n",
    "\n",
    "    unique_filenames = sorted(video_data['video_path'].unique())\n",
    "    test_data = video_data[video_data['video_path'].map(lambda x: x in unique_filenames)]\n",
    "    return test_data\n",
    "\"\"\"\n",
    "def preProBuildWordVocab(sentence_iterator, word_count_threshold=5):\n",
    "    # borrowed this function from NeuralTalk\n",
    "    print('preprocessing word counts and creating vocab based on word count threshold ', (word_count_threshold))\n",
    "    word_counts = {}\n",
    "    nsents = 0\n",
    "    for sent in sentence_iterator:\n",
    "        nsents += 1\n",
    "        for w in sent.lower().split(' '):\n",
    "            word_counts[w] = word_counts.get(w, 0) + 1\n",
    "    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "    print('filtered words from', (len(word_counts), ' to ', len(vocab)))\n",
    "\n",
    "    ixtoword = {}\n",
    "    ixtoword[0] = '<pad>'\n",
    "    ixtoword[1] = '<bos>'\n",
    "    ixtoword[2] = '<eos>'\n",
    "    #ixtoword[3] = '<unk>'\n",
    "\n",
    "    wordtoix = {}\n",
    "    wordtoix['<pad>'] = 0\n",
    "    wordtoix['<bos>'] = 1\n",
    "    wordtoix['<eos>'] = 2\n",
    "    #wordtoix['<unk>'] = 3\n",
    "\n",
    "    for idx, w in enumerate(vocab):\n",
    "        if w not in ['<pad>', '<bos>', '<eos>']:\n",
    "            wordtoix[w] = idx+3\n",
    "            ixtoword[idx+3] = w\n",
    "\n",
    "    word_counts['<pad>'] = nsents\n",
    "    #word_counts['<bos>'] = nsents\n",
    "    #word_counts['<eos>'] = nsents\n",
    "    #word_counts['<unk>'] = nsents\n",
    "\n",
    "    bias_init_vector = np.array([1.0 * word_counts[ ixtoword[i] ] for i in ixtoword])\n",
    "    bias_init_vector /= np.sum(bias_init_vector) # normalize to frequencies\n",
    "    bias_init_vector = np.log(bias_init_vector)\n",
    "    bias_init_vector -= np.max(bias_init_vector) # shift to nice numeric range\n",
    "\n",
    "    return wordtoix, ixtoword, bias_init_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Video_Caption_Generator():\n",
    "    def __init__(self, dim_image, n_words, dim_hidden, batch_size, n_lstm_steps, n_video_lstm_step, n_caption_lstm_step, bias_init_vector=None):\n",
    "        self.dim_image = dim_image                     # 4096\n",
    "        self.n_words = n_words                         # 6900\n",
    "        self.dim_hidden = dim_hidden                   # 256\n",
    "        self.batch_size = batch_size                   # 100\n",
    "        self.n_lstm_steps = n_lstm_steps               # 80\n",
    "        self.n_video_lstm_step=n_video_lstm_step       # 80\n",
    "        self.n_caption_lstm_step=n_caption_lstm_step   # 40\n",
    "\n",
    "        #with tf.device(\"/cpu:0\"):\n",
    "        with tf.device(\"/device:GPU:0\"):              # 6900 x 256 \n",
    "            self.Wemb = tf.Variable(tf.random_uniform([n_words, dim_hidden], -0.1, 0.1), name='Wemb')\n",
    "\n",
    "                                                  # 256\n",
    "        self.lstm1 = tf.contrib.rnn.BasicLSTMCell(dim_hidden, state_is_tuple=False)\n",
    "        self.lstm2 = tf.contrib.rnn.BasicLSTMCell(dim_hidden, state_is_tuple=False)\n",
    "                                                             # 4096 x 256 \n",
    "        self.encode_image_W = tf.Variable( tf.random_uniform([dim_image, dim_hidden], -0.1, 0.1), name='encode_image_W')\n",
    "        self.encode_image_b = tf.Variable( tf.zeros([dim_hidden]), name='encode_image_b')\n",
    "                                                          # 256 x 6900\n",
    "        self.embed_word_W = tf.Variable(tf.random_uniform([dim_hidden, n_words], -0.1,0.1), name='embed_word_W')\n",
    "        if bias_init_vector is not None:\n",
    "            self.embed_word_b = tf.Variable(bias_init_vector.astype(np.float32), name='embed_word_b')\n",
    "        else:                                        # 6900\n",
    "            self.embed_word_b = tf.Variable(tf.zeros([n_words]), name='embed_word_b')\n",
    "\n",
    "    def build_model(self):                 # 50 x 80 x 4096\n",
    "        video = tf.placeholder(tf.float32, [self.batch_size, self.n_video_lstm_step, self.dim_image])\n",
    "        video_mask = tf.placeholder(tf.float32, [self.batch_size, self.n_video_lstm_step])\n",
    "                                           # 50 x 40+1\n",
    "        caption = tf.placeholder(tf.int32, [self.batch_size, self.n_caption_lstm_step+1])\n",
    "        caption_mask = tf.placeholder(tf.float32, [self.batch_size, self.n_caption_lstm_step+1])\n",
    "                                       # (50*80) x 4096\n",
    "        video_flat = tf.reshape(video, [-1, self.dim_image])\n",
    "        image_emb = tf.nn.xw_plus_b( video_flat, self.encode_image_W, self.encode_image_b ) # (batch_size*n_lstm_steps, dim_hidden)\n",
    "                                          # 50\n",
    "        image_emb = tf.reshape(image_emb, [self.batch_size, self.n_lstm_steps, self.dim_hidden])\n",
    "                          # 100 x \n",
    "        state1 = tf.zeros([self.batch_size, self.lstm1.state_size])\n",
    "        state2 = tf.zeros([self.batch_size, self.lstm2.state_size])\n",
    "        padding = tf.zeros([self.batch_size, self.dim_hidden])\n",
    "\n",
    "        probs = []\n",
    "        loss = 0.0\n",
    "\n",
    "        ##############################  Encoding Stage ##################################\n",
    "        for i in range(0, self.n_video_lstm_step):\n",
    "            if i > 0:\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "            #with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "                output1, state1 = self.lstm1(image_emb[:,i,:], state1)\n",
    "\n",
    "            with tf.variable_scope(\"LSTM2\"):\n",
    "            #with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "                output2, state2 = self.lstm2(tf.concat([padding, output1], 1), state2)\n",
    "\n",
    "        ############################# Decoding Stage ######################################\n",
    "        for i in range(0, self.n_caption_lstm_step): ## Phase 2 => only generate captions\n",
    "            #if i == 0:\n",
    "            #    current_embed = tf.zeros([self.batch_size, self.dim_hidden])\n",
    "            #else:\n",
    "            #with tf.device(\"/cpu:0\"):\n",
    "            with tf.device(\"/device:GPU:0\"):\n",
    "                current_embed = tf.nn.embedding_lookup(self.Wemb, caption[:, i])\n",
    "\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "            #with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "                output1, state1 = self.lstm1(padding, state1)\n",
    "\n",
    "            with tf.variable_scope(\"LSTM2\"):\n",
    "            #with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "                output2, state2 = self.lstm2(tf.concat([current_embed, output1], 1), state2)\n",
    "\n",
    "            labels = tf.expand_dims(caption[:, i+1], 1)\n",
    "            indices = tf.expand_dims(tf.range(0, self.batch_size, 1), 1)\n",
    "            concated = tf.concat([indices, labels], 1)\n",
    "            onehot_labels = tf.sparse_to_dense(concated, tf.stack([self.batch_size, self.n_words]), 1.0, 0.0)\n",
    "\n",
    "            logit_words = tf.nn.xw_plus_b(output2, self.embed_word_W, self.embed_word_b)\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit_words, labels=onehot_labels)\n",
    "            cross_entropy = cross_entropy * caption_mask[:,i]\n",
    "            probs.append(logit_words)\n",
    "\n",
    "            current_loss = tf.reduce_sum(cross_entropy)/self.batch_size\n",
    "            loss = loss + current_loss\n",
    "\n",
    "        return loss, video, video_mask, caption, caption_mask, probs\n",
    "    \n",
    "\n",
    "    def build_generator(self):\n",
    "        video = tf.placeholder(tf.float32, [1, self.n_video_lstm_step, self.dim_image])\n",
    "        video_mask = tf.placeholder(tf.float32, [1, self.n_video_lstm_step])\n",
    "\n",
    "        video_flat = tf.reshape(video, [-1, self.dim_image])\n",
    "        image_emb = tf.nn.xw_plus_b(video_flat, self.encode_image_W, self.encode_image_b)\n",
    "        image_emb = tf.reshape(image_emb, [1, self.n_video_lstm_step, self.dim_hidden])\n",
    "\n",
    "        state1 = tf.zeros([1, self.lstm1.state_size])\n",
    "        state2 = tf.zeros([1, self.lstm2.state_size])\n",
    "        padding = tf.zeros([1, self.dim_hidden])\n",
    "\n",
    "        generated_words = []\n",
    "\n",
    "        probs = []\n",
    "        embeds = []\n",
    "\n",
    "        for i in range(0, self.n_video_lstm_step):\n",
    "            if i > 0:\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "            #with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "                output1, state1 = self.lstm1(image_emb[:, i, :], state1)\n",
    "\n",
    "            with tf.variable_scope(\"LSTM2\"):\n",
    "            #with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "                output2, state2 = self.lstm2(tf.concat([padding, output1], 1), state2)\n",
    "\n",
    "        for i in range(0, self.n_caption_lstm_step):\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            \n",
    "            \n",
    "            print('i=',i)\n",
    "            if i == 0:\n",
    "                #with tf.device('/cpu:0'):\n",
    "                with tf.device(\"/device:GPU:0\"):\n",
    "                    current_embed = tf.nn.embedding_lookup(self.Wemb, tf.ones([1], dtype=tf.int64))\n",
    "\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "            #with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "                output1, state1 = self.lstm1(padding, state1)\n",
    "\n",
    "            with tf.variable_scope(\"LSTM2\"):\n",
    "            #with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "                output2, state2 = self.lstm2(tf.concat([current_embed, output1], 1), state2)\n",
    "\n",
    "            logit_words = tf.nn.xw_plus_b( output2, self.embed_word_W, self.embed_word_b)\n",
    "            max_prob_index = tf.argmax(logit_words, 1)[0]\n",
    "            generated_words.append(max_prob_index)\n",
    "            probs.append(logit_words)\n",
    "\n",
    "            #with tf.device(\"/cpu:0\"):\n",
    "            with tf.device(\"/device:GPU:0\"):\n",
    "                current_embed = tf.nn.embedding_lookup(self.Wemb, max_prob_index)\n",
    "                current_embed = tf.expand_dims(current_embed, 0)\n",
    "\n",
    "            embeds.append(current_embed)\n",
    "\n",
    "        return video, video_mask, generated_words, probs, embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var_history = dict()\n",
    "def print_all_vars(msg):\n",
    "    global var_history\n",
    "    #print(msg)\n",
    "    \n",
    "    var_history[msg]= tf.global_variables()\n",
    "    #print('[')\n",
    "    #for a in tf.global_variables():\n",
    "        #print(a)\n",
    "    #print(']')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1450 training set, 100 testing set\n",
      "loading training label ...\n",
      "loading training id ...\n",
      "loading testing label ...\n",
      "loading testing id ...\n",
      "loading training features ...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import glob\n",
    "import random\n",
    "\n",
    "data_dir = './MLDS_hw2_data/'\n",
    "test_out_dir = './test_out_dir/'\n",
    "peer_out_dir = './peer_out_dir/'\n",
    "\n",
    "train_feat_list = pd.Series(glob.glob(data_dir+'training_data/feat/*'))\n",
    "test_feat_list = pd.Series(glob.glob(data_dir+'testing_data/feat/*'))\n",
    "print(len(train_feat_list), 'training set,', len(test_feat_list), 'testing set')\n",
    "\n",
    "\n",
    "import json\n",
    "print('loading training label ...')\n",
    "train_label_json = json.load(open(data_dir+'training_label.json'))\n",
    "print('loading training id ...')\n",
    "training_id = pd.Series([x.split('/')[-1].replace('.npy','') for x in train_feat_list])\n",
    "\n",
    "print('loading testing label ...')\n",
    "test_label_json = json.load(open(data_dir+'testing_label.json'))\n",
    "print('loading testing id ...')\n",
    "testing_id = pd.Series([x.split('/')[-1].replace('.npy','') for x in test_feat_list])\n",
    "\n",
    "\n",
    "    \n",
    "import re\n",
    "\n",
    "#caption_words = set()\n",
    "#caption_dict = {'2':'two', '3':'three', '4':'four', '5':'five', '6':'six'}\n",
    "train_caption_dict = dict()\n",
    "test_caption_dict = dict()\n",
    "captions = list()\n",
    "#words = set()\n",
    "for tj in train_label_json:\n",
    "    tj_caption = list()\n",
    "    for c in tj['caption']:\n",
    "        c = '<bos> '+c+' <eos>'\n",
    "        c = c.replace('one hundred', '100')\n",
    "        c = c.replace('two hundred', '200')\n",
    "\n",
    "            \n",
    "        s = [re.sub('[\\\",.;?!%”“()]', '', s.lower()) for s in c.split(' ') if re.sub('[\\\",.;?!%”“]', '', s.lower()) ]\n",
    "        s = [si.replace('2','two').replace('3','three').replace('4','four').replace('5','five').replace('6','six').replace('three/four','3/4') for si in s]\n",
    "        #words |= set(s)\n",
    "        captions.append(' '.join(s))\n",
    "        tj_caption.append(' '.join(s))\n",
    "    else:\n",
    "        train_caption_dict[tj['id']] = tj_caption\n",
    "        del tj_caption\n",
    "        \n",
    "for tj in test_label_json:\n",
    "    tj_caption = list()\n",
    "    for c in tj['caption']:\n",
    "        c = c.replace('one hundred', '100')\n",
    "        c = c.replace('two hundred', '200')\n",
    "\n",
    "            \n",
    "        s = [re.sub('[\\\",.;?!%”“()]', '', s.lower()) for s in c.split(' ') if re.sub('[\\\",.;?!%”“]', '', s.lower()) ]\n",
    "        s = [si.replace('2','two').replace('3','three').replace('4','four').replace('5','five').replace('6','six').replace('three/four','3/4') for si in s]\n",
    "        #words |= set(s)\n",
    "        captions.append(' '.join(s))\n",
    "        tj_caption.append(' '.join(s))\n",
    "    else:\n",
    "        test_caption_dict[tj['id']] = tj_caption\n",
    "        del tj_caption\n",
    "\n",
    "\n",
    "print('loading training features ...')\n",
    "train_data = list()\n",
    "for train_feat in train_feat_list:\n",
    "    train_data.append(np.load(train_feat))\n",
    "else:\n",
    "    train_data = np.array(train_data)\n",
    "    print('train_data.shape:', train_data.shape, 'train_data.dtype:', train_data.dtype)\n",
    "    print('pickling train_data')\n",
    "    pickle.dump(train_data, open('train_data.pkl', 'wb'))\n",
    "    print('done!')\n",
    "    \n",
    "    \n",
    "print('loading testing features ...')\n",
    "test_data = list()\n",
    "for test_feat in test_feat_list:\n",
    "    test_data.append(np.load(test_feat))\n",
    "else:\n",
    "    test_data = np.array(test_data)\n",
    "    print('test_data.shape:', test_data.shape, 'test_data.dtype:', test_data.dtype)\n",
    "    print('pickling test_data')\n",
    "    pickle.dump(test_data, open('test_data.pkl', 'wb'))\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#def train():\n",
    "## training phase\n",
    "if True:\n",
    "    print_all_vars('before training')\n",
    "    \n",
    "    wordtoix, ixtoword, bias_init_vector = preProBuildWordVocab(captions, word_count_threshold=0)\n",
    "    np.save(\"./data/wordtoix\", wordtoix)\n",
    "    np.save('./data/ixtoword', ixtoword)\n",
    "    np.save(\"./data/bias_init_vector\", bias_init_vector)\n",
    "    \n",
    "    model = Video_Caption_Generator(dim_image=dim_image,                     # 4096\n",
    "                                    n_words=len(wordtoix),                   # 11720\n",
    "                                    dim_hidden=dim_hidden,                   # 1000\n",
    "                                    batch_size=batch_size,                   # 50\n",
    "                                    n_lstm_steps=n_frame_step,               # 80\n",
    "                                    n_video_lstm_step=n_video_lstm_step,     # 80\n",
    "                                    n_caption_lstm_step=n_caption_lstm_step, # 20\n",
    "                                    bias_init_vector=bias_init_vector)       # bias_init_vector.shape: (11720,)\n",
    "    \n",
    "    print_all_vars('training phase after Video_Caption_Generator()')\n",
    "    tf_loss, tf_video, tf_video_mask, tf_caption, tf_caption_mask, tf_probs = model.build_model()\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=200)\n",
    "    with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE) as scope:\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(tf_loss)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    loss_fd = open('loss.txt', 'w')\n",
    "    loss_to_draw = []\n",
    "    \n",
    "    #################################\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        loss_to_draw_epoch = []\n",
    "        index = list(range(train_data.shape[0]))\n",
    "        \n",
    "        np.random.shuffle(index)\n",
    "        train_data_shuffle = np.copy(train_data[index])\n",
    "        training_id_shuffle = pd.Series.copy(training_id[index])\n",
    "        \n",
    "        caption_shuffle = [random.choice(train_caption_dict[i]) for i in training_id_shuffle ]\n",
    "\n",
    "        for start in range(0, 1450, batch_size):\n",
    "            end = start + batch_size\n",
    "\n",
    "                \n",
    "            start_time = time.time()\n",
    "            \n",
    "            current_feats = train_data_shuffle[start:end]\n",
    "            current_video_masks = np.ones((batch_size, n_video_lstm_step))\n",
    "            current_captions = caption_shuffle[start:end]\n",
    "            current_caption_ind = [[wordtoix[ci] for ci in c.split(' ')] for c in current_captions]\n",
    "\n",
    "            current_caption_matrix = sequence.pad_sequences(current_caption_ind, padding='post', maxlen=n_caption_lstm_step)\n",
    "            current_caption_matrix = np.hstack( [current_caption_matrix, np.zeros( [len(current_caption_matrix), 1] ) ] ).astype(int)\n",
    "            current_caption_masks = np.zeros(current_caption_matrix.shape)\n",
    "            nonzeros = np.array( [(x != 0).sum() + 1 for x in current_caption_matrix ])\n",
    "                                \n",
    "            for ind, row in enumerate(current_caption_masks):\n",
    "                row[:nonzeros[ind]] = 1\n",
    "\n",
    "            #print(current_caption_matrix.shape)\n",
    "            probs_val = sess.run(tf_probs, \n",
    "                                 feed_dict={tf_video:current_feats,\n",
    "                                            tf_caption: current_caption_matrix})\n",
    "            _, loss_val = sess.run([train_op, tf_loss], \n",
    "                                   feed_dict={tf_video: current_feats,\n",
    "                                              tf_video_mask : current_video_masks,\n",
    "                                              tf_caption: current_caption_matrix,\n",
    "                                              tf_caption_mask: current_caption_masks})\n",
    "            loss_to_draw_epoch.append(loss_val)\n",
    "            \n",
    "            print('idx: ', start, \" Epoch: \", epoch, \" loss: \", loss_val, ' Elapsed time: ', str((time.time() - start_time)))\n",
    "            loss_fd.write('epoch ' + str(epoch) + ' loss ' + str(loss_val) + '\\n')\n",
    "            \n",
    "        \n",
    "        # draw loss curve every epoch\n",
    "        loss_to_draw.append(np.mean(loss_to_draw_epoch))\n",
    "        plt_save_dir = \"./loss_imgs\"\n",
    "        plt_save_img_name = str(epoch) + '.png'\n",
    "        plt.plot(range(len(loss_to_draw)), loss_to_draw, color='g')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(plt_save_dir, plt_save_img_name))\n",
    "\n",
    "        print_all_vars('saving to'+model_path+'/model_'+str(epoch)+'.ckpt')\n",
    "        if np.mean(loss_to_draw_epoch) == min(loss_to_draw):\n",
    "        #if np.mod(epoch, 20) == 0:\n",
    "        #if np.mod(epoch, 1) == 0:\n",
    "            save_path = saver.save(sess, model_path+'/model_'+str(epoch)+'.ckpt')\n",
    "            print(\"Epoch \", epoch, \" is improved. Saving the model at\", save_path, '...')\n",
    "            #saver.save(sess, os.path.join(model_path, 'model'), global_step=epoch)\n",
    "            #saver.save(sess, model_path+'/model_'+str(epoch)+'.ckpt')\n",
    "        \n",
    "    #################################\n",
    "    \n",
    "    loss_fd.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 0\n",
      "i= 1\n",
      "i= 2\n",
      "i= 3\n",
      "i= 4\n",
      "i= 5\n",
      "i= 6\n",
      "i= 7\n",
      "i= 8\n",
      "i= 9\n",
      "i= 10\n",
      "i= 11\n",
      "i= 12\n",
      "i= 13\n",
      "i= 14\n",
      "i= 15\n",
      "i= 16\n",
      "i= 17\n",
      "i= 18\n",
      "i= 19\n",
      "i= 20\n",
      "i= 21\n",
      "i= 22\n",
      "i= 23\n",
      "i= 24\n",
      "i= 25\n",
      "i= 26\n",
      "i= 27\n",
      "i= 28\n",
      "i= 29\n",
      "i= 30\n",
      "i= 31\n",
      "i= 32\n",
      "i= 33\n",
      "i= 34\n",
      "i= 35\n",
      "i= 36\n",
      "i= 37\n",
      "i= 38\n",
      "i= 39\n",
      "./models/model_1.ckpt\n",
      "#############################\n",
      "INFO:tensorflow:Restoring parameters from ./models/model_1.ckpt\n",
      "#############################\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    model_path='./models/model_1.ckpt'\n",
    "\n",
    "\n",
    "    ixtoword = pd.Series(np.load('./data/ixtoword.npy').tolist())\n",
    "\n",
    "    bias_init_vector = np.load('./data/bias_init_vector.npy')\n",
    "    \"\"\"\n",
    "    model = Video_Caption_Generator(dim_image=dim_image,                     # 4096\n",
    "                                    n_words=len(ixtoword),                   # 11720\n",
    "                                    dim_hidden=dim_hidden,                   # 1000\n",
    "                                    batch_size=batch_size,                   # 50\n",
    "                                    n_lstm_steps=n_frame_step,               # 80\n",
    "                                    n_video_lstm_step=n_video_lstm_step,     # 80\n",
    "                                    n_caption_lstm_step=n_caption_lstm_step, # 20\n",
    "                                    bias_init_vector=bias_init_vector)       # bias_init_vector.shape: (11720,)\n",
    "    \n",
    "    \"\"\"\n",
    "    video_tf, video_mask_tf, caption_tf, probs_tf, last_embed_tf = model.build_generator()\n",
    "    \"\"\"\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    saver = tf.train.Saver() \n",
    "    \"\"\"\n",
    "    print(model_path)\n",
    "    print('#############################')\n",
    "    saver.restore(sess, model_path)\n",
    "    print('#############################')\n",
    "\n",
    "    test_output_txt_fd = open('S2VT_results_256_96_0.01.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_output_txt_fd = open('S2VT_results_512_200.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n",
      "video_feat.shape (1, 80, 4096)\n",
      "a man is is \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#def test(model_path='./models/model_0.ckpt'):\n",
    "if True:\n",
    "    \"\"\"\n",
    "    model_path='./models/model_990.ckpt'\n",
    "    #test_data = get_video_test_data(video_test_data_path, video_test_feat_path)\n",
    "    #test_videos = test_data['video_path'].unique()\n",
    "    \n",
    "    #test_videos = np.array(['./rgb_test_features/klteYv1Uv9A_27_33.avi.npy', \n",
    "    #                    './rgb_test_features/5YJaS2Eswg0_22_26.avi.npy',\n",
    "    #                    './rgb_test_features/UbmZAe5u5FI_132_141.avi.npy',\n",
    "    #                    './rgb_test_features/JntMAcTlOF0_50_70.avi.npy',\n",
    "    #                    './rgb_test_features/tJHUH9tpqPg_113_118.avi.npy',])\n",
    "\n",
    "    ixtoword = pd.Series(np.load('./data/ixtoword.npy').tolist())\n",
    "\n",
    "    bias_init_vector = np.load('./data/bias_init_vector.npy')\n",
    "\n",
    "    \n",
    "    \n",
    "    model = Video_Caption_Generator(dim_image=dim_image,                     # 4096\n",
    "                                    n_words=len(ixtoword),                   # 11720\n",
    "                                    dim_hidden=dim_hidden,                   # 1000\n",
    "                                    batch_size=batch_size,                   # 50\n",
    "                                    n_lstm_steps=n_frame_step,               # 80\n",
    "                                    n_video_lstm_step=n_video_lstm_step,     # 80\n",
    "                                    n_caption_lstm_step=n_caption_lstm_step, # 20\n",
    "                                    bias_init_vector=bias_init_vector)       # bias_init_vector.shape: (11720,)\n",
    "    \n",
    "    \n",
    "    #print_all_vars('testing phase after Video_Caption_Generator()')\n",
    "    video_tf, video_mask_tf, caption_tf, probs_tf, last_embed_tf = model.build_generator()\n",
    "    #print_all_vars('testing phase after model.build_generator()')\n",
    "    ####sess = tf.InteractiveSession()\n",
    "\n",
    "    ####saver = tf.train.Saver()\n",
    "    print(model_path)\n",
    "    print('#############################')\n",
    "    saver.restore(sess, model_path)\n",
    "    print('#############################')\n",
    "    \"\"\"\n",
    "    #test_output_txt_fd = open('S2VT_results.txt', 'w')\n",
    "    #for idx, video_feat_path in enumerate(test_videos):\n",
    "    for tid, video_feat in zip(testing_id, test_data):\n",
    "\n",
    "        #print(idx, video_feat_path)\n",
    "\n",
    "        video_feat = np.copy(video_feat.reshape(1, n_video_lstm_step ,dim_image))\n",
    "        print('video_feat.shape', video_feat.shape)\n",
    "        #video_feat = np.load(video_feat_path)\n",
    "        #video_mask = np.ones((video_feat.shape[0], video_feat.shape[1]))\n",
    "        #video_mask = np.ones(video_feat.shape)\n",
    "        #print('video_feat.shape[1] == n_frame_step:', video_feat.shape[1] == n_frame_step)\n",
    "        if video_feat.shape[1] == n_frame_step:\n",
    "            video_mask = np.ones((video_feat.shape[0], video_feat.shape[1]))\n",
    "        else:\n",
    "            continue\n",
    "        #video_feat = video_feat.reshape(1, n_video_lstm_step, dim_image)\n",
    "            #shape_templete = np.zeros(shape=(1, n_frame_step, 4096), dtype=float )\n",
    "            #shape_templete[:video_feat.shape[0], :video_feat.shape[1], :video_feat.shape[2]] = video_feat\n",
    "            #video_feat = shape_templete\n",
    "            #video_mask = np.ones((video_feat.shape[0], n_frame_step))\n",
    "\n",
    "        generated_word_index = sess.run(caption_tf, feed_dict={video_tf:video_feat, video_mask_tf:video_mask})\n",
    "        generated_words = ixtoword[generated_word_index]\n",
    "\n",
    "        punctuation = np.argmax(np.array(generated_words) == '<eos>') + 1\n",
    "        generated_words = generated_words[:punctuation]\n",
    "\n",
    "        generated_sentence = ' '.join(generated_words)\n",
    "        generated_sentence = generated_sentence.replace('<bos> ', '')\n",
    "        generated_sentence = generated_sentence.replace(' <eos>', '')\n",
    "        ## filter\n",
    "        gsplit = generated_sentence.split(' ')\n",
    "        for gi, g in enumerate(gsplit):\n",
    "            if gi == 0:\n",
    "                g1 = g\n",
    "            else:\n",
    "                if g1 == g:\n",
    "                    gsplit[gi-1] = ''\n",
    "        else:\n",
    "            generated_sentence = ' '.join([gs for gs in gsplit if gs])\n",
    "        \n",
    "        print(generated_sentence,'\\n')\n",
    "        #test_output_txt_fd.write(video_feat_path + '\\n')\n",
    "        test_output_txt_fd.write(tid+','+generated_sentence + '\\n')\n",
    "    test_output_txt_fd.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
