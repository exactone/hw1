{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1450 training set, 100 testing set\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "data_dir = './MLDS_hw2_data/'\n",
    "test_out_dir = './test_out_dir/'\n",
    "peer_out_dir = './peer_out_dir/'\n",
    "\n",
    "train_feat_list = glob.glob(data_dir+'training_data/feat/*')\n",
    "test_feat_list = glob.glob(data_dir+'testing_data/feat/*')\n",
    "print(len(train_feat_list), 'training set,', len(test_feat_list), 'testing set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training features ...\n",
      "train_data.shape: (1450, 80, 4096) train_data.dtype: float64\n",
      "pickling train_data\n"
     ]
    }
   ],
   "source": [
    "print('loading training features ...')\n",
    "train_data = list()\n",
    "for train_feat in train_feat_list:\n",
    "    train_data.append(np.load(train_feat))\n",
    "else:\n",
    "    train_data = np.array(train_data)\n",
    "    print('train_data.shape:', train_data.shape, 'train_data.dtype:', train_data.dtype)\n",
    "    print('pickling train_data')\n",
    "    pickle.dump(train_data, open('train_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading testing features ...\n",
      "test_data.shape: (100, 80, 4096) test_data.dtype: float64\n",
      "pickling test_data\n"
     ]
    }
   ],
   "source": [
    "print('loading testing features ...')\n",
    "test_data = list()\n",
    "for test_feat in test_feat_list:\n",
    "    test_data.append(np.load(test_feat))\n",
    "else:\n",
    "    test_data = np.array(test_data)\n",
    "    print('test_data.shape:', test_data.shape, 'test_data.dtype:', test_data.dtype)\n",
    "    print('pickling test_data')\n",
    "    pickle.dump(test_data, open('test_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train_data, test_data from .pkl ...\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "print('loading train_data, test_data from .pkl ...')\n",
    "train_data = pickle.load(open('train_data.pkl', 'rb'))\n",
    "test_data = pickle.load(open('test_data.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training label ...\n",
      "loading training id ...\n",
      "loading testing label ...\n",
      "loading testing id ...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print('loading training label ...')\n",
    "train_label_json = json.load(open(data_dir+'training_label.json'))\n",
    "print('loading training id ...')\n",
    "training_id = [x.split('/')[-1].replace('.npy','') for x in train_feat_list]\n",
    "\n",
    "print('loading testing label ...')\n",
    "test_label_json = json.load(open(data_dir+'testing_label.json'))\n",
    "print('loading testing id ...')\n",
    "testing_id = [x.split('/')[-1].replace('.npy','') for x in test_feat_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 6346 words in training/testing label\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "caption_words = set()\n",
    "caption_dict = {'2':'two', '3':'three', '4':'four', '5':'five', '6':'six'}\n",
    "for tj in train_label_json + test_label_json:\n",
    "    for c in tj['caption']:\n",
    "        c = c.replace('one hundred', '100')\n",
    "        c = c.replace('two hundred', '200')\n",
    "        s = [re.sub('[\\\",.;?!%”“]', '', s.lower()) for s in c.split(' ')]\n",
    "        \n",
    "        s[-1] = s[-1].replace('.', '')\n",
    "        caption_words |= set(s)\n",
    "else:\n",
    "    caption_words.add('<BOS>')\n",
    "    caption_words.add('<EOS>')\n",
    "    caption_words.add('<NULL>')\n",
    "    \n",
    "    caption_words.remove('2')\n",
    "    caption_words.remove('3')\n",
    "    caption_words.remove('4')\n",
    "    caption_words.remove('5')\n",
    "    caption_words.remove('6')\n",
    "            \n",
    "    print('there are', len(caption_words), 'words in training/testing label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caption_words label encoding...\n",
      "encoding 6346 classes\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "print('caption_words label encoding...')\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(list(caption_words))\n",
    "print('encoding', len(le.classes_), 'classes')\n",
    "#le.transform([1, 1, 2, 6]) \n",
    "#le.inverse_transform([0, 0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 hit at i: 1069 replaced by two\n",
      "2 hit at i: 1449 replaced by two\n",
      "4 hit at i: 659 replaced by four\n",
      "4 hit at i: 659 replaced by four\n",
      "3 hit at i: 25 replaced by three\n",
      "6 hit at i: 931 replaced by six\n",
      "5 hit at i: 1429 replaced by five\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "train_label_json_split = copy.deepcopy(train_label_json)\n",
    "for tid in training_id:\n",
    "    for i, tj in enumerate(train_label_json_split):\n",
    "        if tid == tj['id']:\n",
    "            caption = list()\n",
    "            caption_one_hot = list()\n",
    "            caption_mask = list()\n",
    "            for c in tj['caption']:\n",
    "                c = c.replace('one hundred', '100')\n",
    "                c = c.replace('two hundred', '200')\n",
    "                s = [re.sub('[\\\",.;?!%”“]', '', s.lower()) for s in c.split(' ')]\n",
    "                \n",
    "                # 將2, 3, 4, 5, 6用two, three, four, five, six取代\n",
    "                for si, ss in enumerate(s):\n",
    "                    if ss in caption_dict:\n",
    "                        print(ss, 'hit at i:', i, 'replaced by', caption_dict[ss])\n",
    "                        s[si] = caption_dict[ss]\n",
    "                        \n",
    "                s.insert(0, '<BOS>')\n",
    "                s.append('<EOS>')\n",
    "                for si in range(len(s), 37):\n",
    "                    s.append('<NULL>')\n",
    "                caption.append(s)\n",
    "                s_one_hot = le.transform(s)\n",
    "                caption_one_hot.append(s_one_hot)\n",
    "                caption_mask.append(s_one_hot == le.transform(['<NULL>']))\n",
    "            else:\n",
    "                train_label_json_split[i]['caption'] = caption\n",
    "                train_label_json_split[i]['one_hot'] = caption_one_hot\n",
    "                train_label_json_split[i]['mask'] = caption_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "test_label_json_split = copy.deepcopy(test_label_json)\n",
    "for tid in testing_id:\n",
    "    for i, tj in enumerate(test_label_json_split):\n",
    "        if tid == tj['id']:\n",
    "            caption = list()\n",
    "            caption_one_hot = list()\n",
    "            caption_mask = list()\n",
    "            for c in tj['caption']:\n",
    "                c = c.replace('one hundred', '100')\n",
    "                c = c.replace('two hundred', '200')\n",
    "                s = [re.sub('[\\\",.;?!%”“]', '', s.lower()) for s in c.split(' ')]\n",
    "                \n",
    "                # 將2, 3, 4, 5, 6用two, three, four, five, six取代\n",
    "                for si, ss in enumerate(s):\n",
    "                    if ss in caption_dict:\n",
    "                        print(ss, 'hit at i:', i, 'replaced by', caption_dict[ss])\n",
    "                        s[si] = caption_dict[ss]\n",
    "                        \n",
    "                s.insert(0, '<BOS>')\n",
    "                s.append('<EOS>')\n",
    "                for si in range(len(s), 37):\n",
    "                    s.append('<NULL>')\n",
    "                caption.append(s)\n",
    "                s_one_hot = le.transform(s)\n",
    "                caption_one_hot.append(s_one_hot)\n",
    "                caption_mask.append(s_one_hot == le.transform(['<NULL>']))\n",
    "            else:\n",
    "                test_label_json_split[i]['caption'] = caption\n",
    "                test_label_json_split[i]['one_hot'] = caption_one_hot\n",
    "                test_label_json_split[i]['mask'] = caption_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of caption is 37\n"
     ]
    }
   ],
   "source": [
    "print('max length of caption is', 37)\n",
    "#max_ = 0\n",
    "#for t in train_label_json_split+test_label_json_split:\n",
    "#    if len(t['caption']) > max_:\n",
    "#        max_ = len(t['caption'])\n",
    "#else:\n",
    "#    print('max length of caption is', max_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
